{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scapy.all import *\n",
    "from hashlib import md5\n",
    "import arrow\n",
    "import pandas as pd\n",
    "from parse_rssi import parse_rssi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "capture_files = glob.glob(\"captured_packets/*.cap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_station_id(file_name):\n",
    "    p = re.compile('(?:[0-9a-fA-F]:?){12}')\n",
    "    test_str = file_name\n",
    "    return re.findall(p, test_str)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get packets from all available collects\n",
    "### Probe requests are management frames of type 0x00 with subtype 0x04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packets_total = 0\n",
    "collects = []\n",
    "for capture_file in capture_files:\n",
    "    pcap_reader = PcapReader(capture_file)\n",
    "    packets = []\n",
    "    for pkt in pcap_reader:\n",
    "        packets_total += 1\n",
    "        try:\n",
    "            if (pkt.type == 0 and pkt.subtype == 0x04):\n",
    "                packets.append(pkt)\n",
    "        except:\n",
    "            pass\n",
    "    collects.append((parse_station_id(capture_file), packets))\n",
    "\n",
    "print(f'Processed a total of {packets_total} data packets')\n",
    "print(f'Found {len(packets)} probe requests that we can work with')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print out the start and end times of the collected packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for collect in collects:\n",
    "    packets = collect[1]\n",
    "    pkt_start = packets[1]\n",
    "    pkt_end = packets[-1]\n",
    "    \n",
    "    print(f'Station: {collect[0]}')\n",
    "    print(f'Start  : {arrow.get(pkt_start.time)}')\n",
    "    print(f'End    : {arrow.get(pkt_end.time)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the MAC address from each packet, it's associated RSSI (Received Signal Strength Indication) and the time of capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, collect in enumerate(collects):\n",
    "    packets = collect[1]\n",
    "    near = [((pkt.addr2), arrow.get(pkt.time), parse_rssi(pkt)) for pkt in packets]\n",
    "    collects[num] = collect + (near,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the semi-processed data on a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Use a DataFrame for this instead\n",
    "\n",
    "import csv\n",
    "with open('parsed_packet_data.csv','w') as f:\n",
    "    wr = csv.writer(f, dialect='excel')\n",
    "    wr.writerows([('MAC', 'time', 'RSSI', 'station')])\n",
    "    \n",
    "    # Write one collect at a time\n",
    "    for collect in collects:\n",
    "        # Iterate through the proximity data of the current collect\n",
    "        for near in collect[2]:\n",
    "            # Add the station (ID) data to each proximity reading\n",
    "            near = near + (collect[0],)\n",
    "            wr.writerow(near)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data as a Pandas Dataframe, so we can group and filter it\n",
    "## Also, sort it by time, so we can search for mevements in chronological order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proximity_table = []\n",
    "for collect in collects:\n",
    "    # Iterate through the proximity data of the current collect\n",
    "    for near in collect[2]:\n",
    "        # Add the station (ID) data to each proximity reading\n",
    "        near = near + (collect[0],)\n",
    "        proximity_table.append(list(near))\n",
    "\n",
    "proximity_table = pd.DataFrame(proximity_table, columns=['MAC', 'time', 'RSSI', 'station']).sort_values('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a proximity threshold: any signal stronger than it will be considered a person close to the station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proximity_threshold = 96\n",
    "close_ones = proximity_table[proximity_table['RSSI'] > proximity_threshold]\n",
    "print(f\"Found {len(close_ones.groupby(['station', 'MAC']))} diferent devices near the wi-fi collect stations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proximity_table.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check every device on every station and try to find its first movement in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movements = pd.DataFrame([], columns=['personal_token', 'time', 'origin', 'destination'])\n",
    "\n",
    "for close_one in close_ones.itertuples(index=False):\n",
    "    # Find this same MAC in another station in the future\n",
    "    current_mac = close_one[0]\n",
    "    current_time = close_one[1]\n",
    "    current_station = close_one[3]\n",
    "    \n",
    "    movement_filter = (close_ones['MAC'] == current_mac) \\\n",
    "                       & (close_ones['station'] != current_station) \\\n",
    "                       & (close_ones['time'] > current_time)\n",
    "    \n",
    "    movement_found = close_ones[movement_filter]\n",
    "    \n",
    "    if not movement_found.empty:\n",
    "        movements = movements.append({\n",
    "                'personal_token': current_mac, \n",
    "                'time': movement_found.iloc[0]['time'], \n",
    "                'origin': current_station, \n",
    "                'destination': movement_found.iloc[0]['station']\n",
    "            }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Remove duplicated movements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data about movements in a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movements.to_csv('movements.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
